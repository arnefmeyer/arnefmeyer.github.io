---
title: The neural computations underlying vision during self-motion
layout: default
---

<section class="wrapper style1">
	<div class="container">
			<section class="12u">
				<div class="box post">
					<a href="#" class="image left"><img src="{{ site.baseurl }}/assets/images/mousecam_small.png" alt="" /></a>
					<div class="inner">

						<h3>The neural computations underlying vision during self-motion</h3>

						<p>
							When moving through the environment (what humans and other animals
							permanently do), the brain has to solve a pretty hard problem:
							it has to distinguish those sensory inputs that result from
							self-motion from those that are caused by the motion of external
							bjects.

							For example, when crossing a busy road the patterns of visual
							motion occurring on the retina must be separated
							into self-motion (relative to a stationary environment) and external
							object motion	(e.g., pedestrians, cars, cyclists)	within the
							fraction of a second.

							Failing to do so can have serious consequences.
						</p>
						<p>
							There is increasing evidence that non-visual signals, such as
							motor and vestibular signals, are innervating the early visual
							system (e.g., Meyer et al., Neuron 2018).

							These signals have been hypothesized to constitute the neural
							substrate to distinguish visual	object-motion from self-motion.

							However, the range of signals in a freely moving animal, and how
							they shape visual input is still unclear.
						</p>
						<p>
							My goal is understand the nature of these signals and the role
							(computation) they play in visual perception.

							To reach this goal,	I am combining head and eye tracking in freely
							moving mice withlarge-scale neural recordings (aka neuropixels),
							virtual reality, and computational modeling.
						</p>

						<p>
							<h4>Collaborators</h4>
							<ul>
								<li>Francesco Battaglia (Donders, Nijmegen)</li>
								<li>Jasper Poort (University of Cambridge)</li>
							</ul>
						</p>

						<p>
							<h4>Relevant publications</h4>
							<p>
								<strong>Meyer AF</strong>*, Poort J*, O’Keefe J, Sahani M, Linden JF.
								<a href="https://www.cell.com/neuron/fulltext/S0896-6273(18)30822-5"
								target="_blank">A head-mounted camera system integrates detailed
								behavioral monitoring with multichannel electrophysiology in freely
								moving mice</a>. Neuron, 2018, 100, 46-60.
							</p>
							<p>
								<strong>Meyer AF</strong>, O’Keefe J, Poort J.
								<a href="https://biorxiv.org/cgi/content/short/2020.02.20.957712v1"
								target="_blank">Two distinct types of eye-head coupling in freely
								moving mice</a>. bioRxiv, 2020.
							</p>
						</p>

					</div>
			</section>
		</div>
</section>
